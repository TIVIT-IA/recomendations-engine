version: '3.8'
services:
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-latest
    container_name: tei
    environment:
      - MODEL_ID=intfloat/e5-small
    ports:
      - "8090:80"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3

  ingestor:
    build:
      context: .
      dockerfile: ingestor/Dockerfile
    container_name: ingestor
    env_file: .env
    environment:
      REDIS_URL: redis://redis:6379
      TEI_URL: http://tei:80/embed
      INGEST_HEALTH_PORT: 9001
    depends_on:
      tei:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "9001:9001"   # health + metrics
    restart: on-failure
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    restart: unless-stopped

volumes:
  pgdata: {}
